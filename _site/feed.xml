<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-12-26T15:39:04-05:00</updated><id>http://localhost:4000/</id><title type="html">Lei M. Zhang’s Research Blog</title><subtitle>This is the research blog of Lei M. Zhang, PhD (ECE, U of Toronto).  Topics include deep learning, variational inference, graphical models,  information theory and coding theory. Enjoy!</subtitle><entry><title type="html">An Introduction to Staircase Codes</title><link href="http://localhost:4000/jekyll/update/2017/12/14/staircase.html" rel="alternate" type="text/html" title="An Introduction to Staircase Codes" /><published>2017-12-14T20:00:00-05:00</published><updated>2017-12-14T20:00:00-05:00</updated><id>http://localhost:4000/jekyll/update/2017/12/14/staircase</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/12/14/staircase.html">&lt;p&gt;In today’s post, I’d like to describe a class of error-correcting codes that have been the &lt;a href=&quot;http://ieeexplore.ieee.org/document/6787025/&quot;&gt;focus&lt;/a&gt; of my &lt;a href=&quot;http://ieeexplore.ieee.org/document/8082562/&quot;&gt;research&lt;/a&gt; &lt;a href=&quot;http://ieeexplore.ieee.org/document/7950925/&quot;&gt;career&lt;/a&gt; for the better part of the last 6 years. They are called &lt;a href=&quot;http://ieeexplore.ieee.org/document/6074908/&quot;&gt;Staircase Codes&lt;/a&gt; and were originally invented Professor Kschisching’s research group. There has been some recent interest in using staircase codes as efficient error-correcting codes for next-generation, long-haul fiber-optic communication networks.&lt;/p&gt;

&lt;p&gt;I have implemented a staircase code simulator in C++, available at my &lt;a href=&quot;https://github.com/leiz86/staircase-simulator&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-is-an-error-correcting-code&quot;&gt;What is an error-correcting code?&lt;/h2&gt;

&lt;p&gt;First, let’s quickly review error-correcting codes. Given a particular channel model, e.g., the binary-input additive white Gaussian noise (AWGN) channel, we would like to transmit information (measured in bits) over the channel with arbitrarily low probability of error at the receiver.&lt;/p&gt;

&lt;p&gt;According to information theory, such reliable communication is only achievable while operating below the &lt;strong&gt;capacity&lt;/strong&gt; (or &lt;strong&gt;Shannon limit&lt;/strong&gt;) of the channel, given as the rate bits/channel-use. The purpose of an error-correcting code, then, is to achieve the maximum rate of reliable communication, while operating below the channel capacity.&lt;/p&gt;

&lt;p&gt;Operationally, the channel capacity gives us a convenient upper-bound on the performance of an error-correcting code. In simple terms, a good error-correcting code operates very close to the channel capacity.&lt;/p&gt;

&lt;h2 id=&quot;staircase-codes&quot;&gt;Staircase codes&lt;/h2&gt;

&lt;h1 id=&quot;channel-model&quot;&gt;Channel Model&lt;/h1&gt;

&lt;p&gt;Staircase codes are error-correcting codes designed for the binary-symmetric channel (BSC). The BSC is a simple bit-flipping channel where each transmitted is flipped, independently, with probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. The channel is usually denoted by &lt;script type=&quot;math/tex&quot;&gt;\textrm{BSC}(p)&lt;/script&gt;. The capacity of this simple channel is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
C(p) = 1 + p\log_2(p) + (1-p)\log_2(1-p).
\end{equation*}&lt;/script&gt;

&lt;p&gt;Though simple, the BSC is surprisingly effective at modelling modern, long-haul fiber-optic communication systems. Of course, the error-correcting code is usually the very last block in a very long chain of digital, analog, and optical processing modules in which the channel is not very BSC-like. Nevertheless, at the input of the error-correcting code, the BSC is a good model for the channel.&lt;/p&gt;

&lt;h1 id=&quot;motivating-factors&quot;&gt;Motivating Factors&lt;/h1&gt;

&lt;p&gt;Good error-correcting codes for the BSC has been known for a few decades. First to come to mind are low-density parity-check (LDPC) codes and Turbo codes. The key motivation for the invention of the new class of staircase codes for long-haul fiber-optic communications is the fact that these systems have extremely high bit-rates: on the order of 1, 10, even 100 Gbits/s. Modern error-correcting codes such as LDPC or Turbo codes have significant decoding complexity and data-flow which makes supporting the high bit-rates difficult.&lt;/p&gt;

&lt;p&gt;Staircase codes are designed to minimize decoding complexity and data-flow. Emphasizing these requirements means their performance (in terms of coding gain) is slightly below that of other modern error-correcting codes. However, the decrease in complexity is well-worth the trade-off.&lt;/p&gt;

&lt;p&gt;For a detailed (and quantitative) discussion of the motivating factors behind staircase codes, please refer to the paper &lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/6074908/&quot;&gt;Staircase Codes: FEC for 100 Gb/s OTN&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;code-structure-and-encoding&quot;&gt;Code Structure and Encoding&lt;/h1&gt;

&lt;p&gt;Here is a block diagram of the staircase code structure. The origin of its name is now obvious!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/fig/staircase/staircase.jpg&quot; alt=&quot;staircase diagram&quot; title=&quot;Staircase Code Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To understand this diagram, we first define a &lt;strong&gt;component code&lt;/strong&gt;, which is any systematic binary block code &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; of length &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, information bits &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, and decoding radius &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, denoted by &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}(n,k,t)&lt;/script&gt;. We also assume that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; has a low-complexity decoding algorithm, for example, a syndrome decoder.&lt;/p&gt;

&lt;p&gt;First, we select &lt;script type=&quot;math/tex&quot;&gt;w=n&lt;/script&gt; and arrange &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; copies of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt;, row-by-row, within each consecutive pair of staircase blocks: &lt;script type=&quot;math/tex&quot;&gt;B_i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;B_{i+1}&lt;/script&gt;. Naturally, this requires that &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is even.&lt;/p&gt;

&lt;p&gt;Initially, block &lt;script type=&quot;math/tex&quot;&gt;B_0&lt;/script&gt; is filled with all-zero bits. Since this block carries no information, it is actually not transmitted.&lt;/p&gt;

&lt;p&gt;To encode the next block, arrange information (input) bits row-by-row into the white part of block &lt;script type=&quot;math/tex&quot;&gt;B_1&lt;/script&gt;. Arrange blocks &lt;script type=&quot;math/tex&quot;&gt;B_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B_1&lt;/script&gt; into a &lt;script type=&quot;math/tex&quot;&gt;w \times 2w&lt;/script&gt; matrix, encode each component code &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; using its own systematic encoder. The resulting parity (or check) bits are used to populate the red part of block &lt;script type=&quot;math/tex&quot;&gt;B_1&lt;/script&gt;. For subsequent blocks, given &lt;script type=&quot;math/tex&quot;&gt;B_{i-1}&lt;/script&gt;, the matrix &lt;script type=&quot;math/tex&quot;&gt;B_{i-1}^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B_i&lt;/script&gt; is formed (&lt;script type=&quot;math/tex&quot;&gt;()^T&lt;/script&gt; denotes matrix transpose) and the component code encoding proceeds as above.&lt;/p&gt;

&lt;p&gt;Since the red blocks do not carry information, the code rate of a staircase code is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
R = \frac{w - (n-k)}{w} = 1 - 2(1  - k/n).
\end{equation*}&lt;/script&gt;

&lt;h1 id=&quot;decoding&quot;&gt;Decoding&lt;/h1&gt;

&lt;p&gt;Staircase codes are decoded by using an iterative decoder, operating over several received blocks. Consider the above figure to be a set of 7 received blocks of bits from the BSC. The decoder will operate over these blocks, shift out a decoded block, and shift in a newly received block. Such a decoder is referred to as a &lt;strong&gt;sliding-window decoder&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The iterative decoding process is similar to the encoding process. Consider a sliding window containing &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; blocks. For each &lt;script type=&quot;math/tex&quot;&gt;i \in \{0, \dots, W-2\}&lt;/script&gt;, form the &lt;script type=&quot;math/tex&quot;&gt;w \times 2w&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;B_i^T, \, B_{i+1}&lt;/script&gt; and decode each row of the matrix by an efficient component code decoder. For low decoding data-flow, a syndrome decoder is usually used. An iteration is completed when the end of the window is reached.&lt;/p&gt;

&lt;p&gt;This process is then repeated until either a maximum number of iterations has been reached or no bit-flips occur within the window. At the end of decoding, block &lt;script type=&quot;math/tex&quot;&gt;B_0&lt;/script&gt; is decoded and removed from the window. A new block &lt;script type=&quot;math/tex&quot;&gt;B_W&lt;/script&gt; is then shifted into the window. The process continues indefinitely.&lt;/p&gt;

&lt;h2 id=&quot;a-c-simulator-for-staircase-codes&quot;&gt;A C++ Simulator for Staircase Codes&lt;/h2&gt;

&lt;p&gt;If you are interested in trying out staircase codes as a possible error-correction solution, I have implemented a full simulator for staircase codes in C++, available at my &lt;a href=&quot;https://github.com/leiz86/staircase-simulator&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The only non-standard library that the simulator relies on is a &lt;script type=&quot;math/tex&quot;&gt;\textrm{BCH}(1023, 993, 7)&lt;/script&gt; decoder implementation, currently compiled as a shared library on Linux (Ubuntu 16.04). The staircase code core implementation is fully modular and will be able to use any other component code decoder implementation with minor changes.&lt;/p&gt;

&lt;p&gt;The simulated bit-error rate (BER) and block error-rate (BKER) for a staircase code based on &lt;script type=&quot;math/tex&quot;&gt;\textrm{BCH}(1023, 993, 7)&lt;/script&gt; (shortened by 1), with &lt;script type=&quot;math/tex&quot;&gt;w=511&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W=7&lt;/script&gt; and at most &lt;script type=&quot;math/tex&quot;&gt;6&lt;/script&gt; decoding iterations, is shown in the figure below. The simulations were ran on a quad-core Intel Core I5-5200U CPU clocked at 2.20GHz, for around 40 minutes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/files/fig/staircase/simulation.png&quot; alt=&quot;simulation result&quot; title=&quot;Simulation Results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the results, we observe that the transition between having a large number of decoding errors, i.e., little or no error correction, to virtually no decoding errors, is very sharp. This is known as a &lt;strong&gt;threshold&lt;/strong&gt; of the BSC channel parameter, at which the state of the decoder undergoes a phase transition.&lt;/p&gt;

&lt;p&gt;Due to this sharp transition, it is difficult to fully observe the change in error rates around the threshold, especially given the computation power and run-time used to produce the above example plot. As a reference, the results reported in &lt;a href=&quot;http://ieeexplore.ieee.org/document/6787025/&quot;&gt;Staircase Codes With 6% to 33% Overhead&lt;/a&gt; were simulated on a supercomputing cluster for over 3 months!&lt;/p&gt;

&lt;p&gt;For more details and extensive analysis of the threshold of staircase codes, please refer to the paper &lt;a href=&quot;http://ieeexplore.ieee.org/document/8082562/&quot;&gt;Spatially-Coupled Split-Component Codes with Iterative Algebraic Decoding&lt;/a&gt;. Note that Spatially-Coupled Split-Component codes is a generalization of staircase codes.&lt;/p&gt;</content><author><name>Lei M. Zhang</name></author><summary type="html">In today’s post, I’d like to describe a class of error-correcting codes that have been the focus of my research career for the better part of the last 6 years. They are called Staircase Codes and were originally invented Professor Kschisching’s research group. There has been some recent interest in using staircase codes as efficient error-correcting codes for next-generation, long-haul fiber-optic communication networks.</summary></entry><entry><title type="html">Capsule Networks</title><link href="http://localhost:4000/jekyll/update/2017/11/22/capsules.html" rel="alternate" type="text/html" title="Capsule Networks" /><published>2017-11-22T22:16:00-05:00</published><updated>2017-11-22T22:16:00-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/22/capsules</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/22/capsules.html">&lt;p&gt;In this post, I discuss the paper &lt;a href=&quot;https://arxiv.org/abs/1710.09829&quot;&gt;Dynamic Routing Between Capsules&lt;/a&gt; (CapsNet17) by S. Sabour, N. Frosst, and G. Hinton. For a simple Tensorflow implementation, please see my  &lt;a href=&quot;https://github.com/leiz86/capsules-network&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s start with the two main ideas from the paper: &lt;strong&gt;capsules&lt;/strong&gt; and &lt;strong&gt;dynamic routing&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-are-capsules&quot;&gt;What are Capsules?&lt;/h2&gt;

&lt;p&gt;A capsule is a generalization of a scalar neuron to a vector.&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; be the number of inputs to a standard neuron. Let the set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{I} \triangleq \{0, \dots, N-1\}&lt;/script&gt; be an index set of the inputs &lt;script type=&quot;math/tex&quot;&gt;x_i \in \mathbb{R}&lt;/script&gt; and input weights &lt;script type=&quot;math/tex&quot;&gt;w_i \in \mathbb{R}&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i \in \mathcal{I}&lt;/script&gt;, then the output &lt;script type=&quot;math/tex&quot;&gt;y \in \mathbb{R}&lt;/script&gt; of a standard neuron is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}\label{eqn:neuron}
y = f\left( \sum_{i \in \mathcal{I}} w_i x_i \right) 
\end{equation}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;f: \mathbb{R} \mapsto \mathbb{R}&lt;/script&gt; is a non-linear function such as sigmoid or ReLU.&lt;/p&gt;

&lt;p&gt;We use the notation &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \triangleq (x_0, \dots, x_{d-1})&lt;/script&gt; to denote a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-tuple. If the &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-tuple is an element of a &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;-dimensional vector space, then &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is referred to as a (row) vector. We denote the 2-norm of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\|\mathbf{x}\|&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;There are two types of capsules in the capsule network for the MNIST dataset given in CapsNet17. &lt;strong&gt;Primary capsules&lt;/strong&gt; are intended to represent local features. &lt;strong&gt;Digit capsules&lt;/strong&gt; are intended to represent the digit. Let us first focus on digit capsules.&lt;/p&gt;

&lt;p&gt;Here, assume that an arbitrary digit capsule, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v}&lt;/script&gt;, has the same number of inputs, &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, as the above neuron. Let &lt;script type=&quot;math/tex&quot;&gt;\mathbf{u}_i&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i \in \mathcal{I}&lt;/script&gt; be all of the primary capsules that are possible inputs to &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v}&lt;/script&gt;. We assume that &lt;script type=&quot;math/tex&quot;&gt;\mathbf{u}_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{v}&lt;/script&gt; are elements in vector spaces &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{d_u}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{d_v}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;d_u \neq d_v&lt;/script&gt;, in general.&lt;/p&gt;

&lt;p&gt;Assume that we know the set of &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i \in \mathcal{I}&lt;/script&gt;. These are routing coefficients that is determined by the dynamic routing algorithm (see next section). To define a similar expression as (&lt;script type=&quot;math/tex&quot;&gt;\ref{eqn:neuron}&lt;/script&gt;) for capsules, we need to redefine &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;One simple solution is to replace each &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; by a matrix of dimension &lt;script type=&quot;math/tex&quot;&gt;d_u \times d_v&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;W_i&lt;/script&gt;. Then, replace &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; by a non-linear, vector-input scalar-output function. In CapsNet17, this function is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation*}
g(\mathbf{x}) = \left(1 - \frac{1}{1 + \|\mathbf{x}\|^2}\right) \frac{\mathbf{x}}{\|\mathbf{x}\|}.
\end{equation*}&lt;/script&gt;

&lt;p&gt;Finally, we arrive at the expression for an arbitrary digit capsule&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}\label{eqn:capsule}
\mathbf{v} = g\left(\sum_{i \in \mathcal{I}} c_i \mathbf{u}_i W_i\right).
\end{equation}&lt;/script&gt;

&lt;p&gt;It is no surprise that (&lt;script type=&quot;math/tex&quot;&gt;\ref{eqn:capsule}&lt;/script&gt;) is similar to (&lt;script type=&quot;math/tex&quot;&gt;\ref{eqn:neuron}&lt;/script&gt;). Since, as we mentioned, a capsule is really a generalization of a neuron to a vector.&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;From this perspective, the idea of capsules appears to be a straightforward generalization of the scalar neuron. Are there any good reasons for doing so?&lt;/p&gt;

&lt;p&gt;From CapsNet17 and an &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf&quot;&gt;earlier work&lt;/a&gt; by Prof Hinton, there appear to be two reasons for thinking about vectors rather than scalars.&lt;/p&gt;

&lt;p&gt;First, the pooling operation in convolutional neural networks (CNNs), as a way of obtaining higher-level representations from filter outputs, is sub-optimal:&lt;/p&gt;

&lt;p&gt;The max operation could remove information (i.e., filter outputs that are less than the maximum). Primary capsules attempt to capture a similar higher-level representation &lt;strong&gt;without loss of information&lt;/strong&gt; from convolution filter outputs. By simply aggregating groups of filter outputs into a capsule (a vector), all of the information is captured by different characteristics of the vector. For example, given a capsule &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\| \mathbf{x} \|&lt;/script&gt; is considered to be the probability that the feature represented by &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}&lt;/script&gt; is present in the input image.&lt;/p&gt;

&lt;p&gt;Second, capsules can retain more information about affine transformations of a feature than CNNs with pooling:&lt;/p&gt;

&lt;p&gt;CNNs with max pooling is well-known to be translation . Translation is a type of affine transformation. In general, however, an affine transformation is simply as linear map between two vector spaces. In the case where the vector spaces are the same, for example, &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^d&lt;/script&gt; and a vector is a value of a capsule &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x} \in \mathbb{R}^d&lt;/script&gt;, then any affine transformation &lt;script type=&quot;math/tex&quot;&gt;T : \mathbb{R}^d \mapsto \mathbb{R}^d&lt;/script&gt; with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(\mathbf{x}) = A\mathbf{x} + \mathbf{b}&lt;/script&gt;

&lt;p&gt;simply maps between different values of the capsule.&lt;/p&gt;

&lt;h2 id=&quot;what-is-dynamic-routing&quot;&gt;What is dynamic routing?&lt;/h2&gt;

&lt;p&gt;The idea of dynamic routing introduced in CapsNet17 is, in my opinion, the most interest part of the work. It provides a way of connecting capsules between the primary and digit layers &lt;strong&gt;without using back-propagation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;What it uses is essentially the correlation between different capsule outputs. Recall that each primary capsule &lt;script type=&quot;math/tex&quot;&gt;\mathbf{u}_i&lt;/script&gt; is first multiplied (on the right) by the matrix &lt;script type=&quot;math/tex&quot;&gt;W_i&lt;/script&gt;. Denote this &lt;script type=&quot;math/tex&quot;&gt;d_v&lt;/script&gt;-dimensional vector by &lt;script type=&quot;math/tex&quot;&gt;\hat{\mathbf{u}}_i&lt;/script&gt;. The algorithm then calculates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{u}}_i \mathbf{v}^T&lt;/script&gt;

&lt;p&gt;to update the routing coefficients. Since the calculation of &lt;script type=&quot;math/tex&quot;&gt;c_i&lt;/script&gt; is now recursive, the dynamic routing algorithm is performed for a number of iterations.&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; essentially limits the norm of both vectors to less than or equal to 1, the above project is a measure of how closely matched the two vectors are in &lt;strong&gt;orientation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As shown, the algorithm does not explicitly use back-propagation to find the routing coefficients. However, back-propagation is used to optimize the  parameters involved in the routing calculations. In particular, &lt;script type=&quot;math/tex&quot;&gt;W_i&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;As mentioned, I have written a simple Tensorflow implementation of the MNIST capsule network as described in CapsNet17, located at my &lt;a href=&quot;https://github.com/leiz86/capsules-network&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My implementation was based on the excellent Tensorflow implementations of capsule networks &lt;a href=&quot;https://github.com/naturomics/CapsNet-Tensorflow&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/InnerPeace-Wu/CapsNet-tensorflow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Due to the lack of computing resources, I have not yet been able to fully train my implementation to verify the performance reported in CapsNet17.&lt;/p&gt;

&lt;h2 id=&quot;final-comments&quot;&gt;Final Comments&lt;/h2&gt;

&lt;p&gt;CapsNet17 offers a number of interesting ideas, in particular the generalization of scalar neurons to vector neurons and an alternative way of making layer connections. It appears to be more robust to affine transformations in the original image, rather than just translations.&lt;/p&gt;

&lt;p&gt;The authors do not delve into their choice of network architecture, which leaves me with a some immediate questions. For example, why are the dimensions 8 and 16 used for primary and digit capsules, respectively? Can they both be 8-dimensional vectors? Can the weight matrix be simplified to the identity matrix (or &lt;script type=&quot;math/tex&quot;&gt;w_i * I&lt;/script&gt;) in this case?&lt;/p&gt;

&lt;p&gt;The fact that there is 1 digit capsule for each output class also needs a bit more elaboration. I believe that this is a consequence of the additional mask-and-reconstruction used to regularize the MNIST training.&lt;/p&gt;

&lt;p&gt;This shouldn’t be necessary, however, since the set of digit capsules can be encoded. In this case, only &lt;script type=&quot;math/tex&quot;&gt;\lceil \log_2(10) \rceil&lt;/script&gt; output capsules are necessary. Indeed, if capsules are indeed more efficient than CNNs with pooling, then they should not be used in a one-hot manner in any layer.&lt;/p&gt;

&lt;p&gt;Finally, since capsules learn a different type of representation as CNNs and are more robust to certain affine transformations, are they more robust to adversarial examples? Some &lt;a href=&quot;https://github.com/InnerPeace-Wu/CapsNet-tensorflow&quot;&gt;early experiments&lt;/a&gt; appear to answer in the negative.&lt;/p&gt;

&lt;p&gt;Plenty of fun future experiments with capsules!&lt;/p&gt;</content><author><name>Lei M. Zhang</name></author><summary type="html">In this post, I discuss the paper Dynamic Routing Between Capsules (CapsNet17) by S. Sabour, N. Frosst, and G. Hinton. For a simple Tensorflow implementation, please see my GitHub.</summary></entry><entry><title type="html">First Post!</title><link href="http://localhost:4000/jekyll/update/2017/11/19/first-post.html" rel="alternate" type="text/html" title="First Post!" /><published>2017-11-19T10:19:55-05:00</published><updated>2017-11-19T10:19:55-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/19/first-post</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/19/first-post.html">&lt;p&gt;In this blog, I hope to provide clear and concise explanations of interesting research topics in deep learning. I will mostly be writing in the research paper style that I picked up during my PhD time at UofT. For some samples of such writing, please take a look at the links in the About section.&lt;/p&gt;

&lt;p&gt;Stay posted!&lt;/p&gt;</content><author><name></name></author><summary type="html">In this blog, I hope to provide clear and concise explanations of interesting research topics in deep learning. I will mostly be writing in the research paper style that I picked up during my PhD time at UofT. For some samples of such writing, please take a look at the links in the About section.</summary></entry></feed>